{
  "agent_id": "coder2",
  "task_id": "task_5",
  "files": [
    {
      "filename": "scripts/train_cnn.py",
      "purpose": "Training script for the EfficientNet-B1 classifier using the 20k map dataset with data augmentation and validation.",
      "priority": "medium",
      "dependencies": [
        "torch",
        "torchvision",
        "numpy",
        "pandas",
        "sklearn",
        "matplotlib",
        "tqdm",
        "albumentations",
        "tensorboard"
      ],
      "key_functions": [
        "load_dataset",
        "create_data_loaders",
        "train_epoch",
        "validate_epoch",
        "save_checkpoint",
        "plot_training_curves",
        "export_model"
      ],
      "estimated_lines": 400,
      "complexity": "medium"
    },
    {
      "filename": "utils/metrics.py",
      "purpose": "Computes exploration metrics including coverage percentage, path length, exploration time, and efficiency ratios.",
      "priority": "medium",
      "dependencies": [
        "numpy",
        "pandas",
        "geometry_msgs",
        "nav_msgs",
        "matplotlib"
      ],
      "key_functions": [
        "calculate_coverage",
        "compute_path_length",
        "measure_exploration_time",
        "efficiency_ratio",
        "generate_progression_curves",
        "save_metrics"
      ],
      "estimated_lines": 200,
      "complexity": "medium"
    }
  ],
  "project_info": {
    "project_name": "Saliency_Biased_Frontier_Exploration",
    "project_type": "robotics",
    "description": "A ROS-based autonomous exploration system that uses saliency maps from a CNN-based termination criterion to bias frontier-based exploration strategies. The system identifies saliency areas indicating high-interest regions for exploration and integrates this knowledge into three exploration strategies (nearest-frontier, information-gain, and perfect-information-gain) through a weighted utility function.",
    "key_algorithms": [
      "Frontier_Detection",
      "Grad_CAM_Saliency",
      "CNN_Map_Completion_Classifier",
      "Information_Gain_Estimation",
      "Utility_Function_Optimization"
    ],
    "main_libraries": [
      "torch",
      "torchvision",
      "opencv-python",
      "numpy",
      "scipy",
      "skimage",
      "rospy",
      "nav_msgs",
      "geometry_msgs",
      "sensor_msgs",
      "tf2_ros",
      "actionlib",
      "cv_bridge",
      "message_filters",
      "matplotlib",
      "seaborn",
      "pandas",
      "tqdm",
      "pyyaml"
    ]
  },
  "paper_content": "PDF: cs.RO_2508.10689v1_Biasing-Frontier-Based-Exploration-with-Saliency-A.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nBiasing Frontier\u2013Based Exploration with Saliency Areas\nMatteo Luperto1\u2217, Valerii Stakanov1,2, Giacomo Boracchi2, Nicola Basilico1, and Francesco Amigoni2\nAbstract \u2014 Autonomous exploration is a widely studied prob-\nlem where a robot incrementally builds a map of a previously\nunknown environment. The robot selects the next locations to\nreach using an exploration strategy. To do so, the robot has\nto balance between competing objectives, like exploring the\nentirety of the environment, while being as fast as possible.\nMost exploration strategies try to maximise the explored area\nto speed up exploration; however, they do not consider that\nparts of the environment are more important than others, as\nthey lead to the discovery of large unknown areas. We propose\na method that identifies saliency areas as those areas that\nare of high interest for exploration, by using saliency maps\nobtained from a neural network that, given the current map,\nimplements a termination criterion to estimate whether the\nenvironment can be considered fully-explored or not. We use\nsaliency areas to bias some widely used exploration strategies,\nshowing, with an extensive experimental campaign, that this\nknowledge can significantly influence the behavior of the robot\nduring exploration.\nI. I NTRODUCTION\nExploration, a widely studied problem within the field\nof autonomous mobile robotics, involves a robot building a\nmap of an unknown environment by iteratively planning and\nexecuting a sequence of actions. While some learning-based\napproaches have been recently proposed [1], exploration is\ncustomarily conducted by following a Next Best View (NBV)\napproach [2]: given the current map, the robot computes a\nset of candidate locations to reach and selects, according\nto an exploration strategy , the most promising one. In this\nwork, we consider frontier s, namely the boundaries between\nthe known and the unknown space, as candidate locations\nto reach in order to make progress in exploration [3]. After\nthe robot reaches the most promising frontier, it integrates\nthe acquired knowledge into its map and repeats the process\nuntil a termination criterion is met.\nThe goal of exploration is to eventually build a map of the\nenvironment [4]. However, to obtain such a goal, the robot\nneeds to balance between at least two competing objectives.\nOn the one hand, the map should cover the entirety of the\nenvironment. On the other hand, there is a need to explore the\nenvironment efficiently, thus minimising the exploration time\nand the total path length of the robot. Within this context,\nseveral works [5]\u2013[9] have shown how balancing these two\nneeds is far from straightforward. Exploration strategies that\nprefer to explore first the surroundings of the robot\u2019s location,\nsuch as [3], increase systematically but slowly the mapped\narea. A more aggressive exploration of the environment\nselects highly-promising locations that may be far away\nfrom the robot current location and can result into a high\npercentage of mapped area in early stages of exploration,\nbut also in leaving behind several partially explored regions\nFig. 1: Not all frontiers are equally important for exploration;\nsome can lead to locally finishing the exploration of a room,\nwhile others can lead the robot to an entirely different part\nof the building. We identify the saliency areas as areas of\nhigh interest for exploration, highlighted in light blue, and\nwe integrate this idea into different exploration strategies.\nof the environment, thus forcing the robot to backtrack to\npreviously visited locations in the late stages of the process\n[5].\nThe choice of which exploration strategy is better to use\nis also implicitly influenced by the termination criterion\n[10], which decides when an exploration process should be\nconsidered finished. In some applications, the exploration is\nstopped when the process is no longer acquiring relevant\ninformation, for example, when no frontiers exceeding a\nminimum length can be explored [7]. This is usually done\nwhen the main goal is to have full coverage of the map.\nIn such cases, it is preferable to use exploration strategies\nthat explore first the robot\u2019s surroundings before going to\nother parts of the map. In other operational contexts, such as\nUrban Search and Rescue [11], the exploration process stops\nafter a pre-defined (usually short) amount of time since the\napplication requires quick delivery of (possibly incomplete)\ninformation about the environment to the human rescuers.\nIn those settings, exploration strategies that try to reach first\nhighly-informative frontiers, even if they are far away from\nthe robot\u2019s current location, are commonly employed.\nIn most works, the exploration strategy used by the robot\nand the termination criterion employed to stop the explo-\nration run are considered as two independent methods. As a\nresult, the robot\u2019s behaviour is not affected by the fact that\nthe robot is close (or not) to the end of the exploration.\nSome exploration strategies estimate the quantity of new\narea that is visible by the robot when reaching a frontier,\nunder the assumption that the frontiers that lead to the\nbiggest expected increase in the mapped area are a betterarXiv:2508.10689v1  [cs.RO]  14 Aug 2025\n\n--- Page 2 ---\nchoice for the robot. This estimation is commonly called\ninformation gain . Selecting high information gain frontiers\ndrives the robot towards large frontiers in open regions,\nthereby increasing the likelihood of rapidly expanding the\nmapped area. However, while the information gain takes into\naccount the expected amount of observable area from a target\nlocation, it does not take into account the fact that not all\nthe areas of the environment are equally important [12]. As\nan example, two frontiers, one located in a corridor and the\nother in a small room, can have a similar information gain;\nhowever, exploring the corridor first can lead the robot to a\nwhole new unexplored part of the environment, thus allowing\nthe discovery of other highly informative frontiers. At the\nsame time, the exploration of the frontier inside the room is\nless likely to lead the robot to discover new frontiers.\nIn [10], we have presented a method that stops the\nexploration process if the estimated unmapped portions of the\nenvironment are deemed as uninformative. This is a common\nsituation in late stages of exploration runs, where the robot\nrequires a long time to explore small corners or portions\nof the environment that cover only 1or2%of its total area,\nwithout adding additional knowledge to the mapping process.\nIn this paper, we originally use a by-product of the\nmethod [10] to estimate which portions of the map are most\nrelevant to classify the map as not explored. Intuitively, when\nthere are still multiple frontiers left to explore, we estimate\nthe progress of the exploration process and highlight those\nportions of the maps that are still relevant to explore. We\ncall these areas saliency areas . An example is shown in Fig.\n1: while there are still several frontiers across the whole\nenvironment, we identify that the corridor at the bottom of\nthe map (in light blue) is the most promising one to explore\nto reach new large unexplored parts of the building.\nWe thus propose a method to bias an exploration strategy\nusing the knowledge retrieved from the termination criterion,\nthus allowing the two methods to influence each other. In\nthis way, we modify exploration strategies by adapting their\nchoices to the status of the exploration process. We present\ncounter-intuitive experimental results that, with certain ex-\nploration strategies, adding a negative bias (a cost to pay)\nto visit highly promising frontiers can be a better choice for\nthe robot.\nOur contributions are thus the following: (1) we present a\nmethod to extract saliency areas of interest for exploration,\nby using a by-product of the method [10]; (2) we extend\ndifferent exploration strategies to use the concept of saliency\nareas; (3) we provide an extensive experimental evaluation\nusing simulated environments of how the knowledge of\nsaliency areas can influence and change the behaviour of\nwidely used exploration strategies.\nII. R ELATED WORK\nThe main goal of autonomous exploration is to acquire a\nmap of an initially unknown environment, usually considered\nstatic. A recent survey [13] covers well the broad literature\nwithin this field. Along with the goal to have a complete map,\nthere are often additional desiderata, such as maximising thequality of the map built by the robot [14] or the amount of\nmapped area within a time budget [15].\nExploration strategies commonly rank the frontiers by\nusing a combination of distance from the robot\u2019s current\nposition (a cost the robot needs to pay) and its expected\ninformation gain (IG), namely an estimate of the novel\ninformation about the environment that can be acquired by\nthe robot when visiting a frontier (a gain that the robot wants\nto maximise). However, several works [5]\u2013[9] have shown\nhow balancing these two needs is far from straightforward.\nExploration strategies that prefer to always reach the nearest\nfrontiers [3], thus not considering the information gain, can\nbuild a map that covers most of the environment, but are\nusually slow as they exhaustively explore the surroundings\nof the robot location before moving to more interesting and\ninformative frontiers. Exploration strategies that prefer the\nselection of frontiers with high information gain (while also\nconsidering their distance) are often very fast in exploring\nmost of the areas of the building, but then force the robot\nto backtrack to previously visited locations where frontiers\nwith low information gain have been left behind. Eventually,\nstrategies that prefer early-stage fast exploration can require\na longer time to fully explore the environment than strategies\nthat perform a more systematic exploration [2].\nEarly works estimated the information gain of each fron-\ntier using optimistic approaches, such as assuming that all\nthe unmapped area beyond the frontier is free [16], or by\nusing some heuristics, such as considering longer frontiers\nas more promising, i.e., with higher information gain [2],\n[17].\nSeveral recent methods [6], [18]\u2013[24] focus on predicting\na plausible (yet possibly inaccurate) status of the occupancy\nof the unobserved parts of the environment given its partial\n2D map. While the details of the methods differ significantly,\ntheir general approach can be summarized as follows: the\ncurrent map obtained by the robot is fed to a model,\nwhich exploits the fact that indoor environments present\nsome regularities [25]. The model is then used to predict\na plausible, yet possibly inaccurate, state of the portions\nof the environment that are not mapped by the robot. The\npredicted map is used to inform the exploration process as\nit allows the robot to better estimate the information gain of\neach frontier. As a result, the robot is faster in exploring a\nlarge portion of the area during the early steps of exploration,\nbut at the expense of more backtracking at later stages. The\nprediction of the map is usually done using deep learning\ninpainting methods, such as in [6], [19]\u2013[24], or feature-\nbased ones [18]. To cope with the fact that the predicted map\ncan be inaccurate, some methods produce different estimates\nof possible configurations of the same environment, and use\nthem to get a probabilistic estimate of the information gain\n[22]. A limitation of these works is that, as predicting the\nunseen parts of a map is difficult in real environments or\nwith noisy sensor readings, most of these work are designed,\ntrained, and tested in clean maps of empty environments\nassuming perfect sensing, navigation, and localization of the\nrobot [20], [22]\u2013[24].\n\n--- Page 3 ---\nWhile most research has focused on exploration strategies,\nsome works have pointed out that deciding when to stop\nexploration is a particularly relevant yet under-investigated\nproblem [7], [14]. In [7], the authors present an overview\nof common stopping criteria and discuss their limitations. A\nsimple approach is to stop exploration once a pre-set time\nbudget expires [26], but this method offers no guarantee\nthat the resulting map is complete. Other works propose\nending exploration when no candidate locations remain [3],\nwhich ensures map completeness but often results in long\nexploration times. Some methods stop exploration once a\ncertain percentage of the environment is mapped [15], thus\nrequiring the knowledge of the total size of the environment,\nor when maps generated at different times are nearly identical\n[27], signaling that no new area has been observed. In these\nworks, the exploration strategy is not affected by the fact that\nthe robot is close (or not) to reaching the end of exploration.\nThe termination criterion is intertwined with the explo-\nration strategy in information-theoretic methods, such as\nthose based on map entropy [7], [28]; those methods propose\nstopping exploration when the entropy falls below a threshold\nor saturates, and the same entropy measure is used to drive\nexploration. The tight link between the termination criterion\nand the exploration strategy makes it difficult to combine\nthese approaches with other strategies like frontier-based\nexploration.\nIn our work, we do not use any prediction of the map\nto estimate the information gain, but we use the knowledge\nretrieved from a termination criterion to inform the explo-\nration strategy. As our method relies only on the availability\nof the map, it can be combined with different exploration\nstrategies.\nIII. P ROPOSED METHOD\nIn this paper, we consider an autonomous mobile robot that\nexplores an initially unknown two-dimensional environment\nusing a frontier-based exploration strategy [3]. During explo-\nration, the robot can access an evaluation of the current status\nof the exploration process, which is used as a termination\ncriterion [10].\nThe termination criterion identifies, within the map,\nsaliency areas . We detail this process in Section III-B.\nSaliency areas denote the parts of the environment that are\nmore relevant to explore to reach the goal of ending the\nexploration, thus triggering the termination criterion. The\ngoal of this paper is to investigate whether the knowledge of\nthese areas can inform different exploration strategies.\nTo do so, we focus on three different exploration strategies\nthat represent different families of approaches to frontier-\nbased exploration (Section III-A). We then extend those\nstrategies by adding an additional term to the frontier evalua-\ntion that represents its saliency with respect to the exploration\nprocess as a whole (Section III-C).\nA. Frontier-based exploration\nA robot using a frontier-based exploration iteratively up-\ndates a 2D metric map M, which we assume to be a gridmap, that represents the space that the robot can sense.\nEach cell of Mcan be free, obstacle, or unknown. The\nrobot identifies the set Fof frontiers from the current map\nM. A frontier is a chain of free cells that are adjacent to\nunknown cells. The robot selects the best frontier f\u2217\u2208F\nby maximizing a utility function:\nu(f) =\u03b1\u00b7IG(f)\u2212(1\u2212\u03b1)\u00b7dist(f), (1)\nwhere IG(f)is the frontier\u2019s information gain, dist(f)is\nits centroid\u2019s distance from the robot, and \u03b1\u2208[0,1]is a\nparameter that balances these two factors. Note that while\ndist is a cost, that the robot wants to minimize, IGis a gain\nto be maximized. Both IGanddist are normalized in [0,1]\nusing their maximum and minimum values for the frontiers\ninF.\nAccording to this function, the exploration strategy pri-\noritizes nearby frontiers that are also promising, as these\nare more likely to reveal large areas of unknown space.\nAfter the frontier f\u2217is selected, the robot plans and follows\na path to f\u2217\u2019s centroid, updating the map with the new\nsensory data acquired while traveling. We consider three\ndifferent exploration strategies as representatives of different\napproaches.\nFirst, we consider a nearest-frontier exploration strategy\n(NF) [3], where \u03b1= 0 and thus the information gain is not\nconsidered by the robot when selecting the next frontier to\nvisit. This strategy always selects the closest frontier.\nThen, we consider a second strategy, which uses the space\nthat is potentially observable from the centroid of the frontier\nto estimate the information gain, as in [2], and thus we label\nitIG. Our estimate of the information gain is optimistic,\nas it assumes that the unobserved area beyond each frontier\nand within the robot\u2019s sensor footprint is free and can be\nall observed. This strategy, when compared to NF, quickly\nincreases the area observed by the robot, but eventually\nresults into a longer time required for the full coverage of\nthe environment [9]. We set \u03b1= 0.5.\nFinally, we consider a third strategy, named IG\u2217, that\npredicts the information gain of a frontier by estimating\nmore accurately the status of the map beyond a frontier.\nThis idea has been proposed by several recent approaches\n[6], [18], [21]\u2013[23] that predict the status of the unknown\nparts of the environment and use this prediction to estimate\nthe area that is observable from the centroid of a frontier.\nIn this way, the estimated information gain is more accurate\nand less optimistic than the one made by IG. Empirically,\nthese methods allow a significant speed-up in the early\nstages of exploration runs, but at the cost of backtracking\nto observe frontiers with low information gain at the end\n[5]. We assume that the information strategy IG\u2217has the\nbest possible knowledge, namely we use the full map of the\nenvironment to compute the exact information gain of each\nfrontier, thus simulating a perfect map-prediction method.\nAlso for IG\u2217we set \u03b1= 0.5.\n\n--- Page 4 ---\n(a)\n (b)\n (c)\nFig. 2: Saliency areas at different stages of exploration for environment e1. Full map in (c).\nB. Estimating the progress of the exploration run\nThe robot relies on the method presented in [10] and that\nuses an image depicting the current grid map to detect if\nit satisfactorily represents (or not) the whole environment.\nThe classification output defines the termination criterion.\nWe present here a brief overview of the method; please refer\nto [10] for full details.\nThe classification model is based on a convolutional neural\nnetwork (CNN). The termination criterion is modeled as a\nvisual recognition problem where the classification task is to\ndistinguish between maps that are not-explored yet from\nthose that are sufficiently explored . Starting from a dataset\nof maps, we label as not-explored those maps that a\nhuman (who knows the full map of the environment) would\nconsider incomplete, because they miss areas of interest, like\nentire rooms or relevant portions of the environment. We\nthen consider as explored those maps that a human would\nconsider complete enough to represent the full environment.\nNote that, in maps labelled as explored , some small\nparts of the environments (like corners) may not be entirely\nmapped yet, but such parts are not considered relevant and\nthe mapping process can be stopped even if such portions are\nmissing. Often, the map beyond these remaining frontiers\ncan be easily estimated from the known part of the map\n[20], [29], as in the case where a corner of the room has\nnot been fully explored, but other walls and rooms close to\nit are already known. An example of an explored map is\nshown in Fig. 6.\nWe use as CNN an EfficientNet B1 [30], pre-trained on\nImageNet [31] and fine-tuned on a dataset of about 20,000\nmaps collected during different exploration runs [27], which\nare automatically labelled as explored or not.\nThe output of the method is a label which indicates if\nthe map is not-explored orexplored , and thus if\nthe exploration run should continue or can be stopped. We\nthen leverage saliency maps on the trained CNN classifier to\ndesign new exploration strategies. Saliency maps, like Grad-\n(a)\n (b)\nFig. 3: GRAD-Cam output (a) and saliency areas (b).\nCAM [32], are explainability techniques that provide, for\neach input image, a heatmap that highlights which regions\nhave primarily influenced the network to decide a specific\nclass. Therefore, saliency maps associated with the class\nnot-explored can suggest which regions to prioritize in\nthe exploration, as shown in Fig. 1. We call these regions\nsaliency areas .\nThese saliency areas evolve and change during explo-\nration, as shown in Figs. 3a, 1, and 2a-b, that show four maps\nfrom the same exploration run. In early stages of exploration\n(Fig. 3a), the model highlights most of the frontiers; with the\nprogression of exploration (Fig. 2a-b), only those frontiers\nthat are in unexplored rooms or that led to new unobserved\nparts of the environment are highlighted. Finally, Fig. 2c\nshows the map when it is labeled as explored . We can see\nhow, in different stages of the exploration runs, saliency areas\ncan highlight the most relevant portions of the environment\nto explore: while at early stages of exploration, all frontiers\nare equally relevant, in later stages only those that lead to\nnew parts of the environment are highlighted.\nC. Biasing exploration with saliency maps\nIn order to use saliency areas to inform different explo-\nration strategies, we introduce a new term into the framework\n\n--- Page 5 ---\nFig. 4: Distribution of values of S(f).\n(a)e2\n (b)e3\nFig. 5: Two of the three environments used for experiments.\ndescribed in Eq. 1, namely\nu(f) =\u03b1\u00b7IG(f)\u2212(1\u2212\u03b1)\u00b7dist(f) +\u03b2\u00b7S(f),(2)\nwhere \u03b2is a weight and S(f)evaluates the saliency of the\nfrontier.\nThe output of the Grad-CAM [32] is a mask of the same\nsize as the image of the map with values in [0,1], where the\nnon-relevant areas have value 0. An example of this mask\nis shown in Fig. 3a. A threshold \u03b8= 0.1is applied to the\nGrad-CAM image, setting all values below \u03b8to0. Then, we\nidentify all non-zero areas within the map using connected-\ncomponent analysis, finding the shape of all saliency areas.\nThe result of this thresholding step is shown in Fig. 3b. For\neach saliency area, we find the average value of its Grad-\nCAM and we use that for the entire area. Finally, S(f)\nis computed by projecting the centroid of the frontier in\nthe saliency map and using the value of the corresponding\nsaliency area. Fig. 4 shows the empirical distribution of S(f)\nfor a map, which is in the range [0,0.25]and with an average\nof0.15. Note that frontiers fthat are in a saliency area\nwithS(f) = 0 can still be visited by the robot and are not\ndiscarded. Moreover, the value S(f)of the same frontier f\ncan change when the map is updated, and a new saliency\nmap is computed.\nIV. E XPERIMENTAL EVALUATION\nIn this section, we perform an extensive experimental\nevaluation of how using saliency areas can inform different\nexploration strategies.\nA. Experimental setting\nWe use three different simulated large-scale environments\nfrom the dataset representing the MIT+KTH campuses [27],\nFig. 6: An example of our system running in ROS of a map\nconsidered as explored by our termination criterion. In\nthis case, the saliency map from the last not-explored\nmap is used to calculate S().\n.\nwhose maps are shown in Fig. 2c (environment e1) and\nFig. 5 (environments e2ande3.) We use ROS, with a\nTurtlebot3 robot simulated in STAGE, and equipped with\na 2D LiDAR with 10 m of range and a 360\u25e6field-of-view,\nwith realistic simulated noise on sensor readings and robot\nmotion. We use [33] for SLAM and the ROS navigation\nstack for navigation. Experiments are performed on an i7-\n7700 CPU and a 1060 GPU, on a computer with an i7-7400\nCPU and a 1050 GPU, and on a Ryzen7 5700 CPU with a\n3060Ti GPU. The Grad-CAM from [10] processes the map\nin real-time and is integrated with the ROS navigation stack.\nWe measure the progression (over time) of the percentage\nof covered area Aand distance travelled by the robot. For the\nsake of brevity, we report only the time as it is proportional\nto distance. We indicate with Axthe time required to cover\nx%of the total area of the map. We also record the time \u00afT\nwhen the termination criterion from [10] is triggered. We do\n5 runs in each environment for each configuration, reporting\naverage and standard deviation. We continue the exploration\nuntil all reachable frontiers are explored. Saliency maps are\navailable only when the termination criterion from [10] has\nnot been triggered, as they are obtained for maps labelled as\nnot-explored . After a map is labelled as explored ,\nwe use the last available saliency areas, as they still indicate\nareas where promising frontiers are located. An example is\nshown in Fig. 6.\nB. Experiemental results\nWe evaluate how different values of \u03b2in Eq. 2 influence\nthe performance of the three different exploration strategies\nof Section III-A. We indicate with +S(\u03b2)an exploration\nstrategy that uses saliency maps with a given value for \u03b2.\nTable I shows the results; we highlight in bold the best value\nper exploration strategy, and in blue the best value overall.\nFig. 7 shows the progression over time of Afore1.\nWe test different values for \u03b2={\u22122,1,2,4}. As saliency\nareas represent portions of the environment that are important\nto explore, a positive value of \u03b2in Eq. 2 is biasing the robot\nto maximise the explored area in the short term, thus pushing\n\n--- Page 6 ---\nEnv. Strategy A30 A50 A70 A90 A95 A99\u00afT\ne1NF 289,20 \u00b1118,98 535,60 \u00b1109,07 793,40 \u00b1131,18 1145,20 \u00b1193,76 1365,60 \u00b1331,53 1560,20 \u00b1256,52 -\nNF+S(1) 210,20 \u00b143,81 393,60 \u00b186,71 636,40 \u00b1124,14 935,00 \u00b1107,42 1245,80 \u00b1348,56 1603,40 \u00b1232,08 1389,20\nNF+S(2) 244,20 \u00b150,77 465,40 \u00b1100,21 724,80 \u00b1106,34 1210,60 \u00b1208,06 1367,80 \u00b1140,51 1657,40 \u00b1211,85 1478,60\nNF+S(4) 210,60 \u00b124,39 457,00 \u00b183,54 754,80 \u00b1187,63 1241,20 \u00b1216,32 1479,80 \u00b1165,90 1731,60 \u00b1242,27 1560,60\nNF+S(-2) 198,20 \u00b147,89 457,20 \u00b1169,46 646,00 \u00b1159,82 980,80 \u00b1286,77 1127,40 \u00b1241,33 1282,40 \u00b1226,39 1106,80\nIG 198,20 \u00b145,53 712,80 \u00b1533,56 1139,60 \u00b1514,49 1549,60 \u00b1437,18 1779,60 \u00b1586,78 2257,20 \u00b1801,66 -\nIG+S(1) 244,40 \u00b146,01 403,20 \u00b153,15 554,20 \u00b152,29 855,80 \u00b1101,29 994,00 \u00b1168,52 1379,40 \u00b1184,71 1053,00\nIG+S(2) 257,20 \u00b187,43 486,60 \u00b1116,52 674,80 \u00b178,24 943,20 \u00b1104,29 1077,20 \u00b1105,42 1378,80 \u00b1267,88 1118,60\nIG+S(4) 189,00 \u00b170,95 316,00 \u00b160,66 575,40 \u00b138,23 907,60 \u00b1139,18 1158,60 \u00b1269,09 1774,12 \u00b1327,47 1311,80\nIG\u2217204,00 \u00b147,27 443,33 \u00b1108,51 704,67 \u00b1111,05 1297,33 \u00b1408,39 1410,33 \u00b1490,84 1847,83 \u00b1885,57 -\nIG\u2217+S(2) 230,00 \u00b154,69 530,20 \u00b1163,05 830,80 \u00b1179,28 1233,40 \u00b1231,70 1488,40 \u00b1381,44 1742,80 \u00b1387,60 1586,20\nIG\u2217+S(4) 213,60 \u00b145,07 581,00 \u00b1184,02 952,60 \u00b1253,16 1488,20 \u00b1461,78 1653,40 \u00b1409,25 1916,20 \u00b1572,55 1679,00\ne2NF 117,50 \u00b159,71 160,33 \u00b177,49 214,50 \u00b159,46 407,50 \u00b148,29 491,00 \u00b165,01 580,83 \u00b169,29 -\nNF+S(1) 56,20 \u00b118,34 81,20 \u00b131,55 139,60 \u00b156,15 327,40 \u00b131,85 398,20 \u00b161,72 493,80 \u00b191,81 398,40\nNF+S(2) 48,00 \u00b10,00 84,80 \u00b19,39 210,20 \u00b1109,69 485,80 \u00b1101,44 581,60 \u00b1102,09 673,60 \u00b190,09 581,80\nNF+S(4 64,40 \u00b122,46 122,60 \u00b118,78 197,60 \u00b127,12 511,40 \u00b193,65 611,60 \u00b183,38 753,60 \u00b1121,08 636,60\nNF+S(-2) 47,00 \u00b10,00 59,40 \u00b118,39 71,80 \u00b19,07 270,40 \u00b134,61 328,00 \u00b137,58 456,00 \u00b176,40 286,40\nIG 84,20 \u00b130,71 133,60 \u00b173,34 154,20 \u00b175,77 439,20 \u00b1177,27 513,60 \u00b1175,81 629,20 \u00b1175,41 -\nIG+S(1) 84,20 \u00b172,02 117,40 \u00b192,04 146,20 \u00b180,36 345,00 \u00b153,50 494,00 \u00b166,06 597,00 \u00b169,31 431,80\nIG+S(2) 80,00 \u00b130,99 88,00 \u00b128,99 108,80 \u00b129,35 278,00 \u00b161,11 381,60 \u00b161,06 481,00 \u00b170,55 340,20\nIG+S(4) 89,20 \u00b1162,95 130,60 \u00b194,70 185,20 \u00b163,57 327,00 \u00b145,65 402,40 \u00b138,89 477,40 \u00b143,51 373,00\nIG\u221755,80 \u00b118,57 109,60 \u00b148,82 184,40 \u00b147,66 388,20 \u00b183,15 479,40 \u00b1142,33 720,80 \u00b1185,57 -\nIG\u2217+S(2) 52,20 \u00b19,39 118,40 \u00b147,80 197,60 \u00b153,84 435,40 \u00b161,93 535,40 \u00b191,63 636,00 \u00b1131,33 502,40\nIG\u2217+S(4) 288,56 \u00b19,07 126,60 \u00b131,12 222,40 \u00b168,56 494,40 \u00b197,42 590,00 \u00b1101,86 669,40 \u00b1133,25 510,60\ne3NF 198,67 \u00b143,99 484,17 \u00b1120,75 639,67 \u00b1171,15 1074,17 \u00b1253,13 1300,17 \u00b1264,92 1853,17 \u00b1311,92 -\nNF+S(1) 205,60 \u00b141,26 529,40 \u00b192,33 857,20 \u00b1108,38 1220,20 \u00b1197,36 1522,60 \u00b1241,32 1901,80 \u00b1436,33 1345,40\nNF+S(2) 205,00 \u00b141,33 567,60 \u00b178,15 905,80 \u00b1281,01 1355,80 \u00b1246,25 1736,40 \u00b1316,32 2478,00 \u00b1504,59 1520,00\nNF+S(4) 266,20 \u00b176,58 680,80 \u00b1151,54 871,80 \u00b1208,36 1387,40 \u00b1301,58 1655,20 \u00b1259,20 2380,00 \u00b1678,52 1685,40\nNF+S(-2) 246,80 \u00b176,87 415,40 \u00b194,84 749,00 \u00b1107,90 1091,60 \u00b1202,74 1273,00 \u00b155,62 2592,60 \u00b11177,43 1214,00\nIG 204,40 \u00b156,74 386,00 \u00b1109,12 584,00 \u00b1139,95 1758,20 \u00b11057,47 1876,80 \u00b11026,90 2646,00 \u00b11088,98 -\nIG+S(1) 179,20 \u00b134,51 403,80 \u00b173,61 589,00 \u00b154,46 1090,80 \u00b1180,15 1285,60 \u00b1229,85 1890,00 \u00b1563,90 1216,40\nIG+S(2) 247,00 \u00b177,02 392,60 \u00b171,53 689,00 \u00b1157,87 1174,20 \u00b1145,49 1342,20 \u00b1155,32 1886,20 \u00b1307,13 1385,20\nIG+S(4) 204,80 \u00b182,56 386,00 \u00b1117,33 688,20 \u00b193,78 1029,60 \u00b1221,93 1327,60 \u00b1302,20 2521,80 \u00b1356,76 1397,20\nIG\u2217285,40 \u00b167,76 547,60 \u00b1118,51 792,80 \u00b1131,78 1476,60 \u00b1405,61 1661,20 \u00b1523,74 2785,80 \u00b1840,53 -\nIG\u2217+S(2) 269,20 \u00b1100,18 626,60 \u00b1166,32 897,60 \u00b1204,44 1433,00 \u00b1256,23 1682,00 \u00b1250,60 2666,80 \u00b1429,18 1711,80\nIG\u2217+S(4) 274,00 \u00b176,59 519,60 \u00b149,73 848,40 \u00b1152,19 1353,20 \u00b1273,89 1579,20 \u00b1292,15 2286,20 \u00b1410,90 1428,00\nTABLE I: Area covered Aagainst time for different strategies. In bold the best value per exploration strategy, and in blue\nthe best value overall for each environment.\nFig. 7: Progression of explored area against time for e1.\nthe robot towards unexplored portions of the environment.\nConsequently, positive values of \u03b2reinforce the aggressive\nbehaviour triggered by the IG. Conversely, negative values\nof\u03b2push the robot to fully explore its neighbourhood first,\nsetting up a cost for the robot to explore frontiers with\nhigh saliency, namely frontiers that can lead to explore large\nunvisited parts of the environment.\nOverall, results from Table I show that the use of saliency\nareas can have a relevant, positive effect on all exploration\nstrategies, as it can allow the robot to detect which frontiers\nlead to highly-promising unexplored areas of the environ-\nment from those that are not.\nWhen we consider NF, positive values of \u03b2allows the\nrobot to have a behaviour similar to those of methods usingIG, thus allowing the robot to reach faster higher percentage\nof explored areas (up to A70), but paying the price of having\nto backtrack in later stage of exploration.\nMore interestingly, a negative value of \u03b2=\u22122often\nallows the robot to fully explore the entirety of the envi-\nronment in less time, and it is often the best exploration\nstrategy among the options we tested. This is because NF\nalways selects the closest frontiers, but this can lead the robot\nto move far from its current location (e.g., if the closest\nfrontier is a doorway leading the robot to a new room).\nWith NF+S( \u22122), we penalise those frontiers that lead the\nrobot far away if they open the way to new parts of the\nenvironment, thus promoting a more fine-grained exploration\nof its immediate vicinity, and thus furtherly increasing the\n\u201cconservative\u201d behaviour of NF. These findings confirm\nthose of [9], which recently presented an exploration strategy\nthat explore first the local vicinity of the robot with the effect\nof minimizing the future distance covered by the robot to\nexplore other frontiers. This can be seen in particular in\nthe environments e1ande2. The other environment ( e3)\nis particularly complex, and thus the behaviour of NFand\nNF+S( \u22122)is more similar.\nWhen we consider exploration strategies that use IG, we\ntest only positive values of \u03b2, asIGandS(f)are correlated\nand both push the robot towards the same desiderata of\nselecting highly relevant frontiers first. Negative values of\n\u03b2would compete with IGand override it. We can see\n\n--- Page 7 ---\nhowIG+S( 2)andIG+S( 4)improve in early stages of\nexploration runs and allow the robot to reach up to A90\nin less time than IG. As expected, this behaviour has the\ncost that the robot is forced to backtrack in later stages of\nexploration, thus requiring higher times to reach A99when\ncompared with NF.\nFinally, for the exploration strategy that knows the real\ninformation gain of each frontier, IG\u2217, adding bias to highly\nrelevant frontiers does not improve results. This is because\nthe saliency map S(f)is an estimation of promising areas,\nwhile IG\u2217(f)already provides the true information gain that\nthe robot obtains by visiting the frontier. However, it is also\ninteresting to observe how using an estimated IGwith a high\nvalue of \u03b2(e.g., IG+S( 4)) is often a better choice even if\nthe robot has the full knowledge of the real IG, as in IG\u2217.\nThis furtherly proves how saliency maps can be seen as a\nmore fine-grained method to evaluate the information gain\nof frontiers, as they estimate the fact that frontiers can lead\nin the medium-long term to observe entirely new portions\nof the environment. Finally, we note that the use of the\ntermination criterion allows for a significant reduction in\nthe total exploration time \u00afT, especially when NF+S(-2)\nis used.\nV. C ONCLUSION\nIn this work, we presented a method to estimate the most\npromising areas to visit in a partially explored environment,\nto move the robot towards a faster completion of the explo-\nration. We have shown how these saliency areas can inform\ndifferent exploration strategies to meet different desiderata.\nFuture works involve testing the concept of saliency map\nin combination with map predictive methods [22] and with\nlearning-based approaches [1], as well as an evaluation with\nreal robots.\nREFERENCES\n[1] Y . Cao, T. Hou, Y . Wang, X. Yi, and G. Sartoretti, \u201cARiADNE: A\nreinforcement learning approach using attention-based deep networks\nfor exploration,\u201d in Proc. ICRA , 2023, pp. 10 219\u201310 225.\n[2] H. Gonz \u00b4ales-Ba \u02dcnos and J.-C. Latombe, \u201cNavigation strategies for\nexploring indoor environments,\u201d Int J Robot Res , vol. 21, no. 10-11,\npp. 829\u2013848, 2002.\n[3] B. Yamauchi, \u201cA frontier-based approach for autonomous exploration,\u201d\ninProc. CIRA , 1997, pp. 146\u2013151.\n[4] I. Lluvia, E. Lazkano, and A. Ansuategi, \u201cActive mapping and robot\nexploration: A survey,\u201d Sensors , vol. 21, no. 7, p. 2445, 2021.\n[5] L. Ericson, D. Duberg, and P. Jensfelt, \u201cUnderstanding greediness in\nmap-predictive exploration planning,\u201d in Proc. ECMR , 2021, pp. 1\u20137.\n[6] L. Ericson and P. Jensfelt, \u201cBeyond the frontier: Predicting unseen\nwalls from occupancy grids by learning from floor plans,\u201d IEEE RAL ,\n2024.\n[7] J. Placed and J. Castellanos, \u201cEnough is enough: Towards autonomous\nuncertainty-driven stopping criteria,\u201d Proc. IAV , pp. 126\u2013132, 2022.\n[8] M. Luperto, M. Antonazzi, F. Amigoni, and N. A. Borghese, \u201cRobot\nexploration of indoor environments using incomplete and inaccurate\nprior knowledge,\u201d Robot Auton Syst , vol. 133, 2020.\n[9] L. Ericson, J. Pedro, and P. Jensfelt, \u201cInformation gain is not all you\nneed,\u201d arXiv preprint arXiv:2504.01980 , 2025.\n[10] M. Luperto, M. M. Ferrara, G. Boracchi, and F. Amigoni, \u201cEs-\ntimating map completeness in robot exploration,\u201d arXiv preprint\narXiv:2406.13482 , 2024.\n[11] Y . Liu and G. Nejat, \u201cRobotic urban search and rescue: A survey\nfrom the control perspective,\u201d J Intell Robot Syst , vol. 72, no. 2, pp.\n147\u2013165, 2013.[12] A. Quattrini Li, R. Cipolleschi, M. Giusto, and F. Amigoni, \u201cA\nsemantically-informed multirobot system for exploration of relevant\nareas in search and rescue settings,\u201d Auton Robot , vol. 40, no. 4, pp.\n581\u2013597, 2016.\n[13] D. Brugali, L. Muratore, and A. De Luca, \u201cMobile robots exploration\nstrategies and requirements: A systematic mapping study,\u201d Int J Robot\nRes, p. 02783649241313471, 2025.\n[14] J. Placed, J. Strader, H. Carrillo, N. Atanasov, V . Indelman, L. Carlone,\nand J. A. Castellanos, \u201cA survey on active simultaneous localization\nand mapping: State of the art and new frontiers,\u201d IEEE T Robot ,\nvol. 39, no. 3, 2023.\n[15] F. Amigoni, A. Quattrini Li, and D. Holz, \u201cEvaluating the impact of\nperception and decision timing on autonomous robotic exploration,\u201d\ninProc. ECMR , 2013, pp. 68\u201373.\n[16] A. Bircher, M. Kamel, K. Alexis, H. Oleynikova, and R. Siegwart,\n\u201cReceding horizon next-best-view planner for 3D exploration,\u201d in\nProc. ICRA , 2016, pp. 1462\u20131468.\n[17] F. Amigoni, \u201cExperimental evaluation of some exploration strategies\nfor mobile robots,\u201d in Proc. ICRA , 2008, pp. 2818\u20132823.\n[18] M. Luperto, L. Fochetta, and F. Amigoni, \u201cExploration of indoor\nenvironments through predicting the layout of partially observed\nrooms,\u201d in Proc. AAMAS , 2021, pp. 836\u2013843.\n[19] Y . Katsumata, A. Kanechika, A. Taniguchi, L. El Hafi, Y . Hagiwara,\nand T. Taniguchi, \u201cMap completion from partial observation using the\nglobal structure of multiple environmental maps,\u201d Adv Robot , vol. 36,\nno. 5-6, pp. 279\u2013290, 2022.\n[20] R. Shrestha, F.-P. Tian, W. Feng, P. Tan, and R. Vaughan, \u201cLearned\nmap prediction for enhanced mobile robot exploration,\u201d in Proc. ICRA ,\n2019, pp. 1197\u20131204.\n[21] Y . Tao, E. Iceland et al. , \u201cLearning to explore indoor environments\nusing autonomous micro aerial vehicles,\u201d in Proc. ICRA . IEEE, 2024.\n[22] C. Ho, S. Kim, B. Moon et al. , \u201cMapex: Indoor structure exploration\nwith probabilistic information gain from global map predictions,\u201d\narXiv preprint arXiv:2409.15590 , 2024.\n[23] S. Baek, B. Moon, S. Kim, M. Cao, C. Ho, S. Scherer et al. , \u201cPipe\nplanner: Pathwise information gain with map predictions for indoor\nrobot exploration,\u201d arXiv preprint arXiv:2503.07504 , 2025.\n[24] K. Song, G. Chen, M. Tomizuka, W. Zhan, Z. Xiong, and M. Ding,\n\u201cP2 explore: Efficient exploration in unknown clustered environment\nwith floor plan prediction,\u201d arXiv preprint arXiv:2409.10878 , 2024.\n[25] M. Luperto, A. Quattrini Li, and F. Amigoni, \u201cA system for building\nsemantic maps of indoor environments exploiting the concept of\nbuilding typology,\u201d in Proc. RoboCup , 2013, pp. 504\u2013515.\n[26] C. Leung, S. Huang, and G. Dissanayake, \u201cActive slam in structured\nenvironments,\u201d in Proc ICRA , 2008, pp. 1898\u20131903.\n[27] F. Amigoni, V . Castelli, and M. Luperto, \u201cImproving repeatability of\nexperiments by automatic evaluation of slam algorithms,\u201d in Proc.\nIROS , 2018, pp. 7237\u20137243.\n[28] M. Ghaffari Jadidi, J. Valls Miro, and G. Dissanayake, \u201cSampling-\nbased incremental information gathering with applications to robotic\nexploration and environmental monitoring,\u201d Int J Robot Res , vol. 38,\nno. 6, pp. 658\u2013685, 2019.\n[29] M. Luperto, V . Castelli, and F. Amigoni, \u201cPredicting performance of\nslam algorithms,\u201d arXiv preprint arXiv:2109.02329 , 2021.\n[30] M. Tan and Q. Le, \u201cEfficientnet: Rethinking model scaling for con-\nvolutional neural networks,\u201d in Proc. ICML , 2019, pp. 6105\u20136114.\n[31] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\n\u201cImagenet: A large-scale hierarchical image database,\u201d in Proc. CVPR ,\n2009, pp. 248\u2013255.\n[32] R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, \u201cGrad-cam: Visual explanations from deep networks via\ngradient-based localization,\u201d in Proc. ICCV , 2017, pp. 618\u2013626.\n[33] G. Grisetti, C. Stachniss, and W. Burgard, \u201cImproved techniques for\ngrid mapping with Rao-Blackwellized particle filters,\u201d IEEE T Robot ,\nvol. 23, pp. 34\u201346, 2007.",
  "project_dir": "artifacts/projects/Saliency_Biased_Frontier_Exploration",
  "communication_dir": "artifacts/projects/Saliency_Biased_Frontier_Exploration/.agent_comm",
  "assigned_at": "2025-08-17T20:48:44.477101",
  "status": "assigned"
}